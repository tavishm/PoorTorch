{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edfe02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Torch clone library\n",
    "import random\n",
    "import math\n",
    "\n",
    "class poortorch():\n",
    "\n",
    "    ## Tensor Creation Functions\n",
    "    def rand(shape: list, v_range=(0,1), requires_grad=False):\n",
    "        l = poortorch.tensor._create_list_iterable(shape, random.uniform, v_range[0], v_range[1], dim=0)\n",
    "        return poortorch.tensor(l, requires_grad=requires_grad)\n",
    "\n",
    "    def randn(shape: list, requires_grad=False, mean=0, std=1):\n",
    "        l = poortorch.tensor._create_list_iterable(shape, random.gauss, mean, std, dim=0)\n",
    "        return poortorch.tensor(l, requires_grad=requires_grad)\n",
    "    \n",
    "    def zeros(shape: list, requires_grad=False):\n",
    "        l = poortorch.tensor._create_list_iterable(shape, lambda: 0, dim=0)\n",
    "        return poortorch.tensor(l, requires_grad=requires_grad)\n",
    "    \n",
    "    ## Tensor Editing Functions\n",
    "    def exp(xt: 'poortorch.tensor'):\n",
    "        l = poortorch.tensor._edit_list_iterable(xt.__data__, math.exp)\n",
    "        return poortorch.tensor(l, history=[xt], operator='exp')\n",
    "    \n",
    "    ## Custom functions\n",
    "    def transpose(xt: 'poortorch.tensor', dim0: int, dim1: int):\n",
    "        new_shape = list(xt.shape)\n",
    "        new_shape[dim0], new_shape[dim1] = new_shape[dim1], new_shape[dim0]\n",
    "        bl = poortorch.zeros(new_shape)\n",
    "        l = poortorch._transpose_iterable(xt.__data__, bl.__data__, dim0, dim1)\n",
    "        return poortorch.tensor(l, history=[xt], operator=f'transpose-{dim0}-{dim1}')\n",
    "\n",
    "    @staticmethod\n",
    "    def _transpose_iterable(xl: list, yl: list, dim0: int, dim1: int, pos: list =None):\n",
    "        if isinstance(xl, (int, float)):\n",
    "            target_pos = pos.copy() \n",
    "            target_pos[dim0], target_pos[dim1] = target_pos[dim1], target_pos[dim0]\n",
    "            return poortorch.tensor._variable_swap(yl, target_pos, xl)\n",
    "        else:\n",
    "            if pos is None: pos = []\n",
    "            for i, x_item in enumerate(xl):\n",
    "                pos.append(i)\n",
    "                yl = poortorch._transpose_iterable(x_item, yl, dim0, dim1, pos)\n",
    "                pos.pop() \n",
    "            return yl \n",
    "        \n",
    "    ## Tensor to Number functions\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean(xt: 'poortorch.tensor', dim: int =0):\n",
    "        if len(xt.shape) == 1:\n",
    "            mean = sum(xt.__data__) / len(xt.__data__)\n",
    "            return poortorch.tensor(mean, history=[xt], operator='mean')\n",
    "        else:\n",
    "            raise Exception(\"Mean for 2 axes tensors has not been implemented yet ðŸ˜”\")\n",
    "        \n",
    "\n",
    "    class tensor():\n",
    "        def __init__(self, x: list | int | float, history: list =[], operator: str =None, requires_grad: bool = False):\n",
    "            if isinstance(x, (float, int)): \n",
    "                self.__data__ = [x]\n",
    "            else:\n",
    "                self.__data__ = x\n",
    "\n",
    "            self.shape = self._shape()\n",
    "            self.history = history\n",
    "            self.operator = operator\n",
    "            self.requires_grad = requires_grad\n",
    "            self.grad = None\n",
    "\n",
    "        def backward(self):\n",
    "            if self.operator == '+':           \n",
    "                def _backward(self):\n",
    "                    # Gradients flow back equally in such simple element-wise addition operations. \n",
    "                    if self.grad == None:\n",
    "                        self.history[0].grad = poortorch.tensor([1] * self.history[0].shape[0])\n",
    "                        self.history[1].grad = poortorch.tensor([1] * self.history[0].shape[0])\n",
    "                    else: \n",
    "                        self.history[0].grad = self.grad \n",
    "                        self.history[1].grad = self.grad \n",
    "\n",
    "                    # Calculating gradients for the tensors that zt was made of\n",
    "                    self.history[0].backward()\n",
    "                    self.history[1].backward()\n",
    "                _backward(self)\n",
    "\n",
    "            elif self.operator == 'mean':\n",
    "                def _backward(self):\n",
    "                    # y = x1 + x2 + x3 / (3)\n",
    "                    if len(self.history[0].shape) > 1: raise Exception(\"Backward for mean of 2 axes tensors has not been implemented yet ðŸ˜”\")\n",
    "\n",
    "                    if self.grad == None:\n",
    "                        self.history[0].grad = poortorch.tensor([1/self.history[0].shape[0]] * self.history[0].shape[0])\n",
    "                    else: \n",
    "                        self.history[0].grad = self.grad[0] * poortorch.tensor([1/self.history[0].shape[0]])\n",
    "\n",
    "                    self.history[0].backward()\n",
    "                    \n",
    "                _backward(self)\n",
    "\n",
    "            # Removing grad for tensors that don't need to store grad\n",
    "            for t in self.history:\n",
    "                if not t.requires_grad: t.grad = None\n",
    "                \n",
    "\n",
    "        def __str__(self):\n",
    "            return f\"PoorTorch({str(self.__data__)})\"\n",
    "\n",
    "        def __add__(self, yt: 'poortorch.tensor'): \n",
    "            if self.shape != yt.shape:\n",
    "                raise Exception(\"Given two tensors don't have the same shape ðŸ˜”\")\n",
    "            \n",
    "            return poortorch.tensor(poortorch.tensor._add_iterable(self.__data__, yt.__data__), history=[self, yt], operator='+')\n",
    "        \n",
    "\n",
    "        def __mul__(self, yt: 'poortorch.tensor'): \n",
    "            if self.shape != yt.shape:\n",
    "                raise Exception(\"Given two tensors don't have the same shape ðŸ˜”\")\n",
    "        \n",
    "            return poortorch.tensor(poortorch.tensor._mul_iterable(self.__data__, yt.__data__), history=[self, yt], operator='*')\n",
    "\n",
    "        def __matmul__(self, yt: 'poortorch.tensor'):\n",
    "            if len(self.shape) != 2 or len(yt.shape) != 2:\n",
    "                raise Exception(\"Matrix multiplication is only supported for 2D tensors ðŸ˜”\")\n",
    "            \n",
    "            if self.shape[1] != yt.shape[0]:\n",
    "                raise Exception(f\"Cannot multiply tensors with shapes {self.shape} and {yt.shape} ðŸ˜”\")\n",
    "\n",
    "            rows_self = self.shape[0]\n",
    "            cols_self = self.shape[1] # == rows_yt\n",
    "            cols_yt = yt.shape[0] if len(yt.shape) == 1 else yt.shape[1]\n",
    "\n",
    "\n",
    "            result_data = [[0 for _ in range(cols_yt)] for _ in range(rows_self)]\n",
    "\n",
    "            for i in range(rows_self):\n",
    "                for j in range(cols_yt):\n",
    "                    sum_val = 0\n",
    "                    for k_idx in range(cols_self): # k_idx is the shared dimension\n",
    "                        sum_val += self.__data__[i][k_idx] * yt.__data__[k_idx][j]\n",
    "                        result_data[i][j] = sum_val\n",
    "                \n",
    "            return poortorch.tensor(result_data, history=[self, yt], operator='@')\n",
    "\n",
    "\n",
    "        def _shape(self):\n",
    "            xl_data = self.__data__ # Renamed from i_list as it's internal data, not a direct parameter\n",
    "            return tuple(poortorch.tensor._shape_iterable(xl_data)[::-1])\n",
    "        \n",
    "        @staticmethod\n",
    "        def _create_list_iterable(shape: list, fx, *args, dim: int =0, **kwargs):\n",
    "            if dim+1 == len(shape):\n",
    "                out = []\n",
    "                for _ in range(shape[-1]):\n",
    "                    out.append(fx(*args, **kwargs))\n",
    "                return out\n",
    "            else: \n",
    "                out = []\n",
    "                for _ in range(shape[dim]): # Renamed x to _ as it's not used\n",
    "                    out.append(poortorch.tensor._create_list_iterable(shape, fx, *args, dim=dim+1, **kwargs))\n",
    "                return out\n",
    "\n",
    "        @staticmethod\n",
    "        def _edit_list_iterable(xl: list, fx, *args, **kwargs):\n",
    "            if isinstance(xl, (int, float)):\n",
    "                return fx(xl, *args, **kwargs)\n",
    "            else:\n",
    "                out = []\n",
    "                for xi in xl:\n",
    "                    out.append(poortorch.tensor._edit_list_iterable(xi, fx, *args, **kwargs))\n",
    "                return out\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_iterable(xl: list, yl: list):\n",
    "            if isinstance(xl, (int, float)):\n",
    "                return xl+yl\n",
    "            else:\n",
    "                out = []\n",
    "                for xi, yi in zip(xl, yl):\n",
    "                    out.append(poortorch.tensor._add_iterable(xi, yi))\n",
    "                return out\n",
    "\n",
    "        @staticmethod\n",
    "        def _mul_iterable(xl: list, yl: list):\n",
    "            if isinstance(xl, (int, float)):\n",
    "                return xl*yl\n",
    "            else:\n",
    "                out = []\n",
    "                for xi, yi in zip(xl, yl):\n",
    "                    out.append(poortorch.tensor._mul_iterable(xi, yi))\n",
    "                return out\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def _shape_iterable(xl: list):\n",
    "            if not isinstance(xl, list):\n",
    "                raise Exception(\"Given list does not have a definite shape ðŸ˜”\")\n",
    "\n",
    "            elif all(isinstance(i, (int, float)) for i in xl):\n",
    "                return [len(xl)]\n",
    "            \n",
    "            elif not all(isinstance(i, (int, float, list)) for i in xl):\n",
    "                raise Exception(\"Given list has elements other than list, int or float ðŸ˜”\")\n",
    "            \n",
    "            else:\n",
    "                shape = []\n",
    "                ds = []\n",
    "                for k_item in xl: # Changed from iterating by index to iterating by item\n",
    "                    ds.append(poortorch.tensor._shape_iterable(k_item))\n",
    "\n",
    "                same_shape = all(ds[0] == j for j in ds)\n",
    "                if not same_shape:\n",
    "                    raise Exception(\"Given list does not have a definite shape ðŸ˜”\")\n",
    "                if same_shape:\n",
    "                    shape.extend(ds[0])\n",
    "                    shape.append(len(xl))\n",
    "\n",
    "            return shape     \n",
    "            \n",
    "        @staticmethod\n",
    "        def _variable_splice(xl: list, idx: list):\n",
    "            if len(idx) == 1: idx = idx[0]\n",
    "            if isinstance(idx, int): return xl[idx]\n",
    "            else:\n",
    "                next_depth_list = xl[idx[0]]\n",
    "                return poortorch.tensor._variable_splice(next_depth_list, idx[1:])\n",
    "\n",
    "        @staticmethod\n",
    "        def _variable_swap(xl: list, idx: list, value):\n",
    "            if len(idx) == 1: idx = idx[0]\n",
    "            print(xl, idx)\n",
    "            if isinstance(idx, (int, float)): \n",
    "                xl[idx] = value\n",
    "                return xl\n",
    "            else:\n",
    "                next_depth_list = xl[idx[0]]\n",
    "                xl[idx[0]] = poortorch.tensor._variable_swap(next_depth_list, idx[1:], value)\n",
    "                return xl\n",
    "\n",
    "a = poortorch.tensor([1,2,3], requires_grad=True)\n",
    "b = poortorch.tensor([0,1,2], requires_grad=True)\n",
    "c = a + b\n",
    "d = poortorch.mean(c)\n",
    "\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0376fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87768639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorable_1: (3,) Expected: (3,)\n",
      "tensorable_2: (2, 2) Expected: (2, 2)\n",
      "tensorable_3: (2, 2, 2) Expected: (2, 2, 2)\n",
      "tensorable_4: (2, 2, 2, 2) Expected: (2, 2, 2, 2)\n",
      "tensorable_5: (2, 2, 1) Expected: (2, 2, 1)\n",
      "not_tensorable_1: Error as expected -> Given list does not have a definite shape ðŸ˜”\n",
      "not_tensorable_2: Error as expected -> Given list does not have a definite shape ðŸ˜”\n",
      "not_tensorable_3: Error as expected -> Given list has elements other than list, int or float ðŸ˜”\n",
      "not_tensorable_4: Error as expected -> Given list does not have a definite shape ðŸ˜”\n"
     ]
    }
   ],
   "source": [
    "## Test cases for shape\n",
    "\n",
    "\n",
    "# Lists that can be made into tensors\n",
    "tensorable_1 = [1, 2, 3]\n",
    "tensorable_2 = [[1, 2], [3, 4]]\n",
    "tensorable_3 = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n",
    "tensorable_4 = [[[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]], [[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]]]\n",
    "tensorable_5 = [[[1], [2]], [[3], [4]]]\n",
    "\n",
    "# Lists that cannot be made into tensors\n",
    "not_tensorable_1 = [[1, 2], [3]]  # Ragged inner lists\n",
    "not_tensorable_2 = [[1, 2], [3, [4, 5]]]  # Mixed types in inner lists\n",
    "not_tensorable_3 = [1, [2, 3]]  # Mixed types at top level\n",
    "not_tensorable_4 = [[1, 2], [3, 'a']]  # Non-numeric element\n",
    "not_tensorable_5 = [[1, 2], 3]  # Mixed list and int at top level\n",
    "\n",
    "\n",
    "# Convert tensorable lists to poortorch.tensor and print their shapes\n",
    "tensor_objs = [\n",
    "    poortorch.tensor(tensorable_1),\n",
    "    poortorch.tensor(tensorable_2),\n",
    "    poortorch.tensor(tensorable_3),\n",
    "    poortorch.tensor(tensorable_4),\n",
    "    poortorch.tensor(tensorable_5)\n",
    "]\n",
    "\n",
    "print(\"tensorable_1:\", tensor_objs[0].shape, \"Expected: (3,)\")\n",
    "print(\"tensorable_2:\", tensor_objs[1].shape, \"Expected: (2, 2)\")\n",
    "print(\"tensorable_3:\", tensor_objs[2].shape, \"Expected: (2, 2, 2)\")\n",
    "print(\"tensorable_4:\", tensor_objs[3].shape, \"Expected: (2, 2, 2, 2)\")\n",
    "print(\"tensorable_5:\", tensor_objs[4].shape, \"Expected: (2, 2, 1)\")\n",
    "\n",
    "# Check non-tensorable lists for error\n",
    "not_tensorables = [\n",
    "    #not_tensorable_1,\n",
    "    not_tensorable_2,\n",
    "    not_tensorable_3,\n",
    "    not_tensorable_4,\n",
    "    not_tensorable_5\n",
    "]\n",
    "\n",
    "for idx, item in enumerate(not_tensorables, 1):\n",
    "    try:\n",
    "        t = poortorch.tensor(item)\n",
    "        print(f\"not_tensorable_{idx}: shape={t.shape} (Unexpected: should have failed!)\")\n",
    "    except Exception as e:\n",
    "        print(f\"not_tensorable_{idx}: Error as expected -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c159742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8]\n",
      "[10, 12]\n",
      "[[6, 8], [10, 12]]\n",
      "Addition result shape: (2, 2) Expected: (2, 2)\n",
      "Addition result data: [[6, 8], [10, 12]] Expected: [[6, 8], [10, 12]]\n",
      "[[1, 2], [3, 4]] + [[5, 6], [7, 8]]\n",
      "Expected error with incompatible shapes: Given two tensors don't have the same shape! ðŸ˜”\n"
     ]
    }
   ],
   "source": [
    "# Test tensor addition\n",
    "\n",
    "# Create two tensors of the same shape\n",
    "tensor_a = poortorch.tensor([[1, 2], [3, 4]])\n",
    "tensor_b = poortorch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "try:\n",
    "    result = tensor_a + tensor_b\n",
    "    print(\"Addition result shape:\", result.shape, \"Expected: (2, 2)\")\n",
    "    print(\"Addition result data:\", result.__data__, \"Expected: [[6, 8], [10, 12]]\")\n",
    "    print(result.history[0].__data__, result.operator, result.history[1].__data__)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# Test adding tensors with different shapes\n",
    "tensor_c = poortorch.tensor([1, 2, 3])\n",
    "try:\n",
    "    incompatible_result = tensor_a + tensor_c\n",
    "    print(\"Should not reach here as the shapes are incompatible\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected error with incompatible shapes: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
